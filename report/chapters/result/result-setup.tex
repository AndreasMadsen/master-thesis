
\section{Experiment Setup}

\subsubsection{Continues evaluation}
The model evaluations such as misclassification rate and BLEU score calculations that are continuously calculated during training, are done on a randomly sampled mini-batch from the test dataset. The predictions are the output calculated using a typical greedy algorithm, this corresponds to having a beam size of 1.

The evaluations are smoothed using an exponential moving average, this removes most of the noise associated with training neural networks. Because the moving average is done over diffrent samples from the test and training dataset, the moving average will likely also be closer to the true average. However this is not statically guaranteed as the model differes in each iteration and this stationarity is not guaranteed.

\subsubsection{Example predictions}
The predictions on examples from the test dataset are done using a beam search algorithm with a beam size of 10. The prediction shown is the most likely sequence found in the beam search that contains an \texttt{<eos>} symbol.

\subsubsection{Multiple GPU parallelization}
A mini-batch size of 16 observations pr. GPU was used. In some experiments multiple GPUs were used in parallel to speed up convergence, in these case the mini-batch was split evenly to each GPU. The gradients was then calculated independently on each GPU, the average gradients over each GPU-mini-batch was then calculated on the CPU and used to update the model, after which the new parameters are redistributed to the GPUs. This method of GPU parallelization is called \textit{synchronized weight update}, this is not the fastest approach but is the most stable approach \cite{citation-needed}.
