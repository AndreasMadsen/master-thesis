
\section{Experiment Setup}

\subsubsection{Continues evaluation}
Model evaluation measures, such as the misclassification rate, and the BLEU score, are continuously calculated during training using a randomly sampled mini-batch, from the test dataset. The predictions are calculated using a vanilla greedy algorithm, this corresponds to having a beam size of 1.

The evaluations are smoothed using an exponential moving average, this removes most of the noise associated with training neural networks with mini-batches. Because the moving average is done over different samples from the test and training dataset, the moving average will likely be close to the true average. However, this is not statistically guaranteed as the model differs in each iteration, thus stationarity is not guaranteed.

\subsubsection{After training prediction}
The predictions made after training are done using a BeamSearch algorithm with a beam size of 10. Each prediction is the most likely sequence found during the BeamSearch that contains an \texttt{<eos>} symbol.

\subsubsection{Multiple GPU parallelization}
A mini-batch size of 16 observations per GPU is used. In some experiments, multiple GPUs were used in parallel to speed up convergence, in these experiments the mini-batch was split evenly to each GPU. The gradients were then calculated independently on each GPU, the average gradients over each GPU-mini-batch were then calculated on the CPU and used to update the model. After that, the new parameters are redistributed to the GPUs. This method of GPU parallelization is called \textit{synchronized weight update}, this is not the fastest approach but is the most stable approach. The other class of approaches are called \textit{asynchronous weight update} \cite{async-sgd}, besides being less stable and more difficult to implement, DTU doesn't have the resources to justify such an approach.

\subsubsection{Bucket batches}

If a long sequence and a short sequence are used in the same mini-batch, the short sequence will be padded with a lot of zeros. Such padding is wasteful as the loss is masked. To prevent obsessive padding, each sequence pair is partitioned into a set of buckets, each bucket contains sequences of approximately the same length.

Each bucket is defined as a sequence-length interval, where the length of a sequence pair is defined as the max length of the two sequences. Only after the buckets are created are the observations assigned to a bucket.

The partition algorithm first constructs a histogram using the length of each sequence pair. It then greedily partitions the histogram, ensuring that the number of observations in each bucket is more than $2 \cdot \text{batch-size}$, this is to ensure some randomness. It also ensures that the length interval for each bucket is at least 10 characters, this to prevent an unnecessary amount of buckets.

\begin{algorithm}[H]
  \caption{Bucket partition algorithm, outputs length intervals of buckets.}
  \begin{algorithmic}[1]
    \Function{BucketPartitioner}{$\mathcal{D}, minsize, minwidth$}
      \Let{$interval_{left}$}{0}
      \Let{$interval_{size}$}{0}
      \Let{$buckets$}{\Call{Stack}{}}

      \For{$(interval_{right}, size)$ in \Call{Histogram}{$\mathcal{D}$}}
        \If{$length - interval_{left} < minwidth$} \Comment{Bucket is to short}
           \Let{$interval_{size}$}{$interval_{size} + size$}
        \ElsIf{$interval_{size} < minsize$} \Comment{Bucket is to short}
           \Let{$interval_{size}$}{$interval_{size} + size$}
        \Else
           \State \Call{Push}{$buckets, [interval_{left}, interval_{right}]$} \Comment{Accept bucket}
           \Let{$interval_{left}$}{$interval_{right} + 1$} \Comment{Prepare next bucket}
           \Let{$interval_{size}$}{0}
        \EndIf
      \EndFor
      \LineComment{Extend last bucket to contain the remaining dataset.}
      \State \Return{$buckets$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\subsubsection{Software and Hardware}

\textbf{The thesis code is available at: } \url{https://github.com/AndreasMadsen/master-thesis}

TensorFlow 1.1 \cite{tensorflow2015-whitepaper} and Python 3.6 were used to build, train, and evaluate the models. A TensorFlow abstraction layer called \texttt{sugartensor} (\url{https://github.com/buriburisuri/sugartensor}) developed by KakaoBrain was also used. During this thesis I have made 6 separate contributions to the \texttt{sugartensor} project. The plots are generated using ggplot2 \cite{ggplot2} in R.

TensorFlow was compiled for the specific CPU architecture and for CUDA Compute Capabilities 6.1. CUDA 8.0 and CuDNN 5.1 was used.

The DTU HPC system was used for computation. They have two Titan GPU machines, each machine have:
\begin{itemize}[noitemsep]
\item 4 Nvidia Titan X (Pascal) GPUs.
\item 1 Intel Xeon CPU E5-2640 v4 CPU, with 10 cores.
\item 128 GB RAM.
\end{itemize}
