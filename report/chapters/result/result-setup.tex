
\section{Experiment Setup}

\subsubsection{Continues evaluation}
Model evaluation measures, such as the misclassification rate, and the BLEU score, are continuously calculated during training using a randomly sampled mini-batch, from the test dataset. The predictions are calculated using a vanilla greedy algorithm, this corresponds to having a beam size of 1.

The evaluations are smoothed using an exponential moving average, this removes most of the noise associated with training neural networks with mini-batches. Because the moving average is done over different samples from the test and training dataset, the moving average will likely be close to the true average. However, this is not statistically guaranteed as the model differs in each iteration, thus stationarity is not guaranteed.

\subsubsection{Example predictions}
The predictions made after training are done using a BeamSearch algorithm with a beam size of 10. Each prediction is the most likely sequence found during the BeamSearch that contains an \texttt{<eos>} symbol.

\subsubsection{Multiple GPU parallelization}
A mini-batch size of 16 observations per GPU is used. In some experiments, multiple GPUs were used in parallel to speed up convergence, in these cases the mini-batch was split evenly to each GPU. The gradients were then calculated independently on each GPU, the average gradients over each GPU-mini-batch were then calculated on the CPU and used to update the model. After that, the new parameters are redistributed to the GPUs. This method of GPU parallelization is called \textit{synchronized weight update}, this is not the fastest approach but is the most stable approach. The other class of approaches are called \textit{asynchronous weight update} \cite{async-sgd}, besides being less stable and more difficult to implement, DTU does't have the resources to justify such an approach.

\subsubsection{Bucket batches}

If a long sequence and a short sequence are used in the same mini-batch, the short sequence will be padded with a lot of zeros. Such padding is wasteful as the loss is masked. To prevent obsessive padding, each sequence pair is partitioned into a set of buckets, each bucket contains sequences of approximately the same length.

The length of a sequence pair is defines as the max length of the two sequences. Each bucket is defined as a sequence-length interval. Only after the buckets are created are the observations assigned to a bucket.

The partition algorithm first constructs a histogram using the length of each sequence pair. It then greedily partitions the histogram, ensuring that the number of observations in each bucket is more than $2 \cdot \text{batch-size}$, this is to ensure some randomness. It also ensures that the length interval for each bucket is at least 10 characters, this to prevent an unnecessary amount of buckets.

\begin{algorithm}[H]
  \caption{Bucket partition algorithm, outputs length intervals of buckets.}
  \begin{algorithmic}[1]
    \Function{BucketPartitioner}{$\mathcal{D}, minsize, minwidth$}
      \Let{$interval_{left}$}{0}
      \Let{$interval_{size}$}{0}
      \Let{$buckets$}{\Call{Stack}{}}

      \For{$(interval_{right}, size)$ in \Call{Histogram}{$\mathcal{D}$}}
        \If{$length - interval_{left} < minwidth$} \Comment{Bucket is to short}
           \Let{$interval_{size}$}{$interval_{size} + size$}
        \ElsIf{$interval_{size} < minsize$} \Comment{Bucket is to short}
           \Let{$interval_{size}$}{$interval_{size} + size$}
        \Else
           \State \Call{Push}{$buckets, [interval_{left}, interval_{right}]$} \Comment{Accept bucket}
           \Let{$interval_{left}$}{$interval_{right} + 1$} \Comment{Prepare next bucket}
           \Let{$interval_{size}$}{0}
        \EndIf
      \EndFor
      \LineComment{Extend last bucket to contain the remaining dataset.}
      \State \Return{$buckets$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

