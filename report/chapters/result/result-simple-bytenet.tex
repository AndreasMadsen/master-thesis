\section{Simplified ByteNet}

The implementation of the ByteNet model spends $37\%$ of its time waiting for data transfer between the GPU and CPU, and even more time waiting because of the TensorFlow service. This is because of the many of weights and operations that are used in the ByteNet model.

To solve this issue, a simplified version of the ByteNet model is used. This model uses fewer weights and fewer operations than ByteNet, but remains true to the principals behind ByteNet. Those principals are: running in linear time, parallelize over both sequence and observations, and have a resolution persistent encoding. The simplified version also maintains the bottleneck of 200 dimensions that ByteNet has in its dilated-convolution layer.

The idea is simple, if the initial embedding dimensionality is set to 200, then the compression and decompression layers that exist before and after the dilated convolution are not needed. Of cause these layers add non-linearities and weights to the model, thus one should not expect the model to perform equally well. However, the model should predict almost as well as the ByteNet model, while being significantly faster.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[scale=1]{theory/bytenet-small-residual-block.pdf}
        \caption{Residual Block used in encoder.}
    \end{subfigure}
    ~ %
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[scale=1]{theory/bytenet-small-residual-block-causal.pdf}
        \caption{Residual Block used in decoder.}
    \end{subfigure}
    \caption{The residual blocks used in the simplified ByteNet model. The blocks are denoted by Encoder-Residual-Block$(k,r)$ and Decoder-Residual-Block$(k,r)$, where $k$ is the kernel width and $r$ is the dilation rate.}
    \label{fig:result:simple-bytenet:residual-block}
\end{figure}

In terms of weights, the simplified ByteNet model has approximately $\sfrac{5}{9}$ times fewer weights than the ByteNet model. The simplified ByteNet model also has approximately one-third of the operations as the ByteNet model. Based on these parameters one should expect less transfer time and less time spent waiting for the TensorFlow service.

The simplified ByteNet model was validated identically to the how the ByteNet model was validated. The results (appendix \ref{appendix:result:bytenet-small}) were very similar with some minor differences. Memorizing WMT NewsTest took fewer iterations, this is likely because there are fewer parameters. The simplified ByteNet model learned the synthetic digits problem better, with a misclassification error of $0.25$. The improved misclassification error is likely because the simplified ByteNet model has fewer parameters and non-linear transformations, which makes it overfit less.

\subsection{Performance Profiling}

To compare the computational performance of the simplified ByteNet model with the normal ByteNet model, the performance experiment was repeated using the simplified ByteNet model. This experiment learns the WMT NewsTest dataset over 300 epochs. Both a 1 GPU and a 4 GPU setup was used in the experiments.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-small/timing-gpus.pdf}
    \caption{Comparing observations per second, depending on the number of GPUs used.}
    \label{fig:result:simple-bytenet:timing-gpus}
\end{figure}

Measuring the total time spent, and the processed observations per second (figure \ref{fig:result:simple-bytenet:timing-gpus}) reveals that the simplified ByteNet model is faster. On 1 GPU the simplified ByteNet model is about $20\%$ faster.

On 1 GPU there is no data transfer of weights, thus it is only the TensorFlow service and the computation that takes time. Reducing the number of operations to $\sfrac{1}{3}$ should thus result in a $\sfrac{2}{3}$ computational performance gain. This is of course an approximation as there is still the embedding layer, the concatenation of encoding and decoding, and the final output layer, thus $\sfrac{2}{3}$ the operations is an upper bound. However, $20\%$ is still far from what one would expect.

Comparing the time spent in the 1 GPU experiment and the 4 GPU experiment, and because very little data transfer happens when using 1 GPU. The data shows that approximately $40\%$ time is spent transferring data. This is approximately the same as the $37\%$ in the normal ByteNet model. The number of weights is reduced by $\sfrac{5}{9}$, thus one would expect the transfer time to be reduced by this order, but this is not the case.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-small/profile-grouped-gpu4.pdf}
    \caption{Shows time spent executing each part of the ByteNet model, this excludes the waiting time. Each part exists in a hierarchy, which is visualized as levels. Bottom level is the backward and forward pass. Second level is the encoder and decoder. Last level primarily splits the simplified ByteNet Residual Blocks.}
    \label{fig:result:simple-bytenet:profile-grouped}
\end{figure}

Finally, the TensorFlow profiler can be used to investigate what takes time. When processed the results are similar to those from the normal ByteNet model, see figure \ref{fig:result:simple-bytenet:profile-grouped}.

Figure \ref{fig:result:simple-bytenet:profile-grouped} shows that much of the time is spent in the \textit{pre-normalization layer}. The \textit{pre-normalization layer} is the layer in the residual block that contains the first normalization and ReLU activation. This validates the idea that it is the number of operations and not the operations themselves that cost in terms of time. The dilated convolution (called \textit{conv-dilated}) is a much more complex operation than normalization or ReLU, but TensorFlow implements it using just a few operations, thus it doesn't consume that much time. As mentioned earlier the TensorFlow team is aware of this performance issue. TensorFlow is planning to solve this issue by automatically compiling GPGPU (CUDA or OpenCL) kernels that combine multiple operations into one. However, this is still very experimental and doesn't yet provide a performance benefit when using multiple GPUs \cite{google-xla}.

\clearpage
\subsection{WMT Translation Task}

The Europarl v7 dataset is used to train the simplified ByteNet model. The setup is identical to that previously used to train the normal ByteNet model. The internal dimensionality is 400, the Adam optimizer is used with a learning rate of 0.0003, and the mini-batch size is $4 \cdot 16 = 64$ observations, 4 GPUs with synchronized updates was used, and the model ran 13 epochs over the Europarl v7 dataset.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-small/europarl.pdf}
    \caption{Shows BLEU score and cross entropy loss for the Simplified ByteNet model, trained on Europarl v7 and tested on WMT NewsTest 2015. Both training and test measures are calculated on a randomly sampled mini-batch from each dataset. The exponential smoothing used a forget factor of $0.05$. The raw data for the normal ByteNet model is not shown.}
    \label{fig:result:bytenet-small:europarl}
\end{figure}

The simplified ByteNet model uses almost 24 hours less training time. However, it also learns at a slower rate, thus in terms of BLUE score given the time spent learning, the normal ByteNet model is actually much faster.

An interesting observation is that the simplified ByteNet model overfits a lot more than the normal ByteNet model. This contradicts most intuition about overfitting, which usually is that more weights, layers, and non-linearities there is, the easier it is to make the model overfit. The simplified ByteNet model has less of all three factors, thus there is no reason to expect the simplified ByteNet model to overfit more.

It is not clear why this overfitting happens, but it is very likely a contributing factor to why the test BLEU score isn't better. However, this can not be the entire explanation, as also the training BLEU score is worse. If anything, overfitting should cause a higher training BLEU score, as regularization typically increases the training loss in order to decrease the test loss.

Possible regularization methods are L1, L2, and dropout. Particularly, L1 regularization could be interesting in the first 5 \textit{residual blocks}, as these layers likely perform some characters-to-word encoding. Such an encoding should be very sparse, L1 regularization would enforce such a sparsity.

The translations as shown in table \ref{table:result:bytenet-small:translations} are rather bad, they have little connection to the source sequence and there is a tendency for the sequence ``Mr \textit{insert name}'' to appear without context. The poor translations can likely be attributed to the severe overfitting.

Calculating the BLEU score after training on the entire WMT NewsTest dataset, yields a BLEU score of $0.55$. This matches the poor translations a lot better than the sampled BLEU score of approximately $3$.

\begin{table}[h]
\centering
\begin{tabular}{l|r|p{10cm}}
0 & source & Die formelle Anerkennung der Arbeitsrechte als Menschenrechte - und die Erweiterung des Schutzes der B체rgerrechte als Schutz gegen Diskriminierung bei der Schaffung von Arbeitnehmervertretungen - ist l채ngst 체berf채llig. \\[0.1cm]
& target & The formal recognition of labor rights as human rights - and the extension of civil rights protections to prevent discrimination against labor organizing - is long overdue. \\[0.1cm]
& translation & However, the recommendation of jobs and generations and proposes the promotion of gross regions of the world growth to prodictively rejecting the rejection of the representative of the representative increase' . \\[0.1cm]
& BLEU & 0.00 \\[0.1cm] \hline

1 & source & Die Premierminister Indiens und Japans trafen sich in Tokio. \\[0.1cm]
& target & India and Japan prime ministers meet in Tokyo \\[0.1cm]
& translation & Mr Pieper and the independence of Singapore in London. \\[0.1cm]
& BLEU & 0.00 \\[0.1cm] \hline

2 & source & Er wird beschuldigt, am 7. Juni 2013 eine Frau im Scotland's Hotel in Pitlochry in Perthshire vergewaltigt zu haben. \\[0.1cm]
& target & He is alleged to have raped a woman at the Scotland's Hotel in Pitlochry in Perthshire on June 7, 2013. \\[0.1cm]
& translation & Madam President, I should like to contribute to Mr President Mr President in his report. \\[0.1cm]
& BLEU & 0.00 \\[0.1cm]
\end{tabular}
\caption{Cherry-picked translations from WMT NewsTest.}
\label{table:result:bytenet-small:translations}
\end{table}

