\section{SELU ByteNet}

The Simplified ByteNet experiments indicates that, the compression and decompression layers are necessary, and the normalization layers are very expensive. From these observations it makes sense to analyse whether the normalization layers are actually necessary for the network to converge.

The experiment in figure \ref{fig:result:selu-bytenet:bytenet-nonorm-wmt} is the ``Memorizing WMT NewsTest'' experiment on the ByteNet model without normalization layers. The experiment ran for 300 epochs.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-nonorm/validation-memorize-wmt.pdf}
    \caption{Shows BLEU score and cross entropy loss for the German to English WMT NewsTest dataset using the ByteNet model without any normalization layers. The exponential moving average used a forget factor of $0.1$.}
    \label{fig:result:selu-bytenet:bytenet-nonorm-wmt}
\end{figure}

Figure \ref{fig:result:selu-bytenet:bytenet-nonorm-wmt} shows that the non-normalized ByteNet model never completely memorizes the training dataset as it should. It is possible that it could still learn actual translation when trained on the Europarl v7 dataset, however, it is not very likely.

Recently a new paper showed that it is possible to create a ``self-normalizing neural network''. This means that normalization isn't done explicitly by a normalization layer, but instead the network is created such that the parameters will convergence to weights that ensures normalized internal values.

The ``self-normalizing neural network'' was achieved by using a different activation function and weight initialization. Primarily it is the activation function that is responsible for making the network \textit{self-normalizing}, the initialization is primarily just to ensure a reasonable starting point \cite[https://arxiv.org/pdf/1706.02515.pdf]{selu}.

The activation function is a variation on the exponential-linear-unit (ELU), which is similar to the ReLU activation function. The difference is that two constants ($\lambda, alpha$) are added:
\begin{equation}
\mathrm{SELU}(x) = \lambda \begin{cases}
  x & x > 0 \\
  \alpha (\mathrm{exp}(x) - 1) & x \le 0
\end{cases},\quad \text{where: } \begin{array}{c}
  \alpha = 1.6732632423 \\
  \lambda = 1.0507009873
\end{array}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{theory/selu-activation-function.pdf}
    \caption{Shows $\mathrm{ReLU}(x)$ and $\mathrm{SELU}(x)$ in the range $x \in [-5, 5]$.}
    \label{fig:result:selu-bytenet:bytenet-selu-activation}
\end{figure}

The initialization should then be done such that $\mathbb{E}[z_{h_\ell}] = 0$ and $\mathrm{var}[z_{h_\ell}] = 1$. This is achieved by initializing the weights such that:
\begin{equation}
\mathrm{Var}[w_{h_{\ell-1}, h_{\ell}}] = \frac{1}{H_{\ell-1}} \quad \Rightarrow \quad r = \sqrt{\frac{3}{H_{\ell-1}}}
\end{equation}
where $r$ is the symmetric uniform distribution parameter, similarly to that used in He-Initialization.

Using the SELU activation function and its derived initialiser, the convergence for the ``Memorizing WMT NewsTest'' experiment is somewhat similar to the normal ByteNet case (figure \ref{fig:result:selu-bytenet:bytenet-selu-wmt}). However, there are some significant differences. The initial cross entropy loss is much higher in the SELU ByteNet case, this indicates that the derived initialiser isn't the optimal choice. Secondly the training BELU score shows almost no improvement initially and then a very fast improvement after a few 75 epochs. These two observations are likely connected, if the initialization is bad it will take a long time for the network to reach a similar state as the normal ByteNet model does initially. If this is true, and if it's possible to initialize the SELU ByteNet model better, the SELU ByteNet model should actually converge in much fewer iterations that the normal ByteNet model.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-selu/validation-memorize-wmt.pdf}
    \caption{Shows BLEU score and cross entropy loss for the German to English WMT NewsTest dataset using the SELU ByteNet. The exponential moving average used a forget factor of $0.1$.}
    \label{fig:result:selu-bytenet:bytenet-selu-wmt}
\end{figure}

Finally, the synthetic digits experiment shows similar results as the normal ByteNet model (Appendix \ref{appendix:result:bytenet-selu}).

\clearpage
\subsection{Performance Profiling}

Repeating the performance experiment from both the normal ByteNet model and the simplified ByteNet model, shows that the SELU ByteNet model is extremely fast in comparison. Comparing the time spend running 300 epochs is actually a problem in this case, as the heating phase that transfers data and optimizes allocation takes up most of the time. However, comparing obs./sec shows that the SELU model is at least twice as fast as the normal ByteNet model.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-selu/timing-gpus.pdf}
    \caption{Comparing observations per second, depending on the number of GPUs used. The experiment learns the WMT NewsTest dataset over 300 epochs.}
    \label{fig:result:selu-bytenet:timing-gpus}
\end{figure}

The processed profiling in figure \ref{fig:result:selu-bytenet:profile-grouped} (the unprocessed plot is in appendix \ref{appendix:result:bytenet-selu}), shows that the convoluted dilation is the most expensive part. This is completely reasonable, as this is the operation that involves the largest amount of raw computation. However, TensorFlow actually has an inefficient implementation of the dilated convolution, because dilated convolution isn't supported directly by CuDNN v5.1. CuDNN stands for CUDA Deep Neural Network and is a library developed by Nvidia that contains efficient implementation of common operations used in neural networks. The next version of CuDNN supports dilated convolution, but TensorFlow does not yet use this version \cite{nvidia-cudnn}.

Another interesting observation when looking at the processed profiling in figure \ref{fig:result:selu-bytenet:profile-grouped}, is the time spend in the ``\textit{pre-normalization}'' layer, which now just contains the SELU activation, compared to the time spend in \textit{recover-dim}, which just contains a \textit{sequential-dense} layer. This shows that the SELU activation uses an unreasonable amount of time. The SELU activation uses less raw computations than the \textit{sequential-dense} layer, and the SELU activation is purely an element-wise operation and should thus be easy to parallelize. The reason for this performance is the before-mentioned naive execution of the computational graph.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-selu/profile-grouped-gpu4.pdf}
    \caption{Shows time spend executing each part of the ByteNet model, this excludes the waiting time. Each part exists in a hierarchy, which is visualized as levels. Bottom level is the backward and forward pass. Second level is the encoder and decoder. Last level primarily splits the SELU ByteNet Residual Blocks.}
    \label{fig:result:selu-bytenet:profile-grouped}
\end{figure}

\clearpage
\subsection{WMT Translation Task}

The Europarl v7 dataset is used to train the SELU ByteNet model. The setup is identical to that previously used to train the normal and simplified ByteNet model.

\begin{figure}[h]
    \centering
    \includegraphics[scale=1]{bytenet-selu/europarl.pdf}
    \caption{Shows BLEU score and cross entropy loss for the SELU ByteNet model, trained on Europarl v7 and tested on WMT NewsTest 2015. Both training and test measures are calculated on a randomly sampled mini-batch from each dataset. The exponential smoothing used a forget factor of $0.05$. The raw data for the non-simplified ByteNet model is not shown.}
    \label{fig:result:bytenet-selu:europarl}
\end{figure}

From figure \ref{fig:result:bytenet-selu:europarl} it is very apparent that the SELU model is extremely fast in comparison to the normal ByteNet model, it completes all 13 epochs in less than a day. However, it also learns extremely poorly.

The cross entropy on the training dataset spikes on occasion, this could explain some of the poor learning behavior. Such an issue is typically solved with gradient clipping, where a hard upper bound is set on the gradient, preventing an exploding gradient. However, the idea behind the SELU activation function is precisely to prevent vanishing and exploding gradients, thus it doesn't seam reasonable to employ additional techniques to prevent exploding gradients.

Another possibility is that the Adam optimization parameters need to be much different when the SELU activation function is used. However, the Adam optimizer acts under most conditions as a trust-region, thus the parameters shouldn't affect the output too much \cite{adam-optimization}.

It is possible that with enough tuning SELU could be made functional, in which case SELU ByteNet would be a very powerful model. However, the behaviour of SELU and why it theoretically and sometime in practice works, is not obvious. The proof for the convergence properties of SELU alone, is 90+ pages \cite{selu}. This makes the SELU activation function hard to reason about, and thus it is hard to make educated guesses about how to tune the model.

Finally, it is possible that the sparse nature of ReLU is essential for creating a natural language model, where sparse pattern often occur. It is thus possible that the SELU activation function will never work.