%!TEX root = ../Thesis.tex
\chapter{Introduction}

In the European Parliament, there are 23 officially spoken languages and precise translation is essential for clear communication. Each the elected member is not expected to understand all 23 languages, and there isn't a common language that everyone speaks equally well, thus the European Parliament employs translators for translating between these languages \cite{europarl-translation}.

For the European Parliament it is possible to employ translation specialists, and in cases where there aren't someone to translate directly English, French and German are used as relay languages \cite{europarl-translation}. However, translation specialists are not often available to the public.

In India, more than 400 million have internet access, and most of Indiaâ€™s online content is written in English. However, only 20\% of the Indian population speaks English. Excluding English, the nine most used languages in India are: Hindi, Bengali, Marathi, Gujarati, Punjabi, Tamil, Telugu, Malayalam and Kannada. For translating between these languages and English Google have recently started to use Neural Machine Translation in their Google Translate service \cite{google-translate-india}.

Google Translate translates more than 100 billion words each day, for its 500 million users \cite{google-translate-stats}. In the European Parliament, and the United Nations, automatic translation software is also used for assisting the specialists \cite{europarl-translation}. These tools are using for the majority of languages, a technology called ``Statistical machine translation''.

Statistical machine translation (SMT) combines a probability model for the target language, as well as a probability model for mapping between the source and target language. Such a probability model can be a Hidden Markov Model and they will be fitted using a bilingual dataset \cite{smt-comparetive-study}. Bilingual means the dataset contains both the source and target language for each sentence. Previously these models have been word-based, allowing very limited context understand. Recently phrase-based machine translation (PBMT) have been introduced, this allows for much better translation of idioms or multi-word expressions than what has previously been possible. Phrase-based translation is currently the primary strategy used in machine translation. However, even phrase-based translation has a limited understanding of context and can't consider an entire sentence.

Recently advances have been made in applying neural networks to machine translation, this strategy is called neural machine translation (NMT) and has been shown to outperform the current PBMT approaches \cite{google-translate-gnmt}. This approach is able to consider an entire sentence or more and is thus able to understand the context of each word on a level that has not been previously possible.

Beyond being able to process entire sentences, neural machine translation is a more flexible approach than PBMT. As such the NMT strategy is highly relevant for language pairs that have previously been notoriously difficult, such as Chinese to English translation. In September 2016, Google announced that they now used neural machine translation for Chinese to English translation, they are calling this architecture GNMT.

Over the last year, GNMT has been enabled for English, French, German, Spanish, Portuguese, Chinese, Japanese, Korean, Turkish, Russian, Hindi, Vietnamese, Hebrew, and Arabic along with the Indian dialects. These languages are chosen either because there is a huge dataset or because they are notoriously difficult to translate using the existing PBMT strategy. In NMT the quality of the translation is often more dependent on how much data is used, than how advanced the neural architecture is.

In this thesis, NMT will be used to provide German to English translation, and additional effort will be given to applying NMT to ``small'' bilingual dataset. The strategy for applying NMT to ``small'' datasets, is based on the fact that humans, especially babies, are capable of learning languages without prior knowledge. It should thus be possible to use additional non-translated data (monolingual) to train the NMT model. A now almost classical example of using monolingual data to build a language model is the Word2Vec model, this uses the Wikipedia corpus for a single language to create word embeddings. These word embeddings have shown meaningful properties like synonyms being close to each other and relations like $king - man + woman \approx queen$ \cite{word2vec}.

The approach used in this thesis is based on the intuition that translating from for example German to English and then back to German again, should result in the same sentence. This approach does not require the correct English sentence to be known for all sentences, thus a monolingual German dataset can be used. The approach has been shown to yield some improvements \cite{semi-supervised}. This must of cause be used in combination with a bilingual dataset, otherwise the neural network will just learn the identity function. This combination of monolingual and bilingual data is called semi-supervised learning.

There are other approaches for semi-supervised learning, in particular, generative adversarial networks (GAN) has shown good results in computer vision. However, for natural language processing such as NMT, it still lags far behind likelihood based methods \cite{gan-on-nlp}.

The semi-supervised approach does on a theoretical level not depend on the neural translation model. A popular translation model is the Bahdanau Attention model that has shown state-of-the-art performance in machine translation \cite{bahdanau-2015-nmt}. However, the Bahdanau Attention model is a word-based model and for some languages, like German that likes to combine words, it can be advantageous to use characters instead of words as input to the model. While word-based models are currently superior, using characters instead of words eliminates the out-of-dictionary problem that often exists in word-based models.

The out-of-dictionary problem happens because a neural network can only supports a fixed number of outputs, one must thus decide on a fixed dictionary before training the model. Typically 80000 words are used as the dictionary size, if the dictionary becomes much larger than this, it will not be computationally feasible to train the model. 80000 words are not enough to contain every street name and rarely used words, an obvious solution is thus to use characters instead of words. A typical language will not use more than 300 different characters.

While it is possible to use the Bahdanau Attention model with characters as input, the sequences become much longer and this creates certain computational problems. Recently a different approach called ByteNet has been created, this model is specifically character-based and solves the computational problems \cite{bytenet}. The ByteNet model promises high computational performance and achieves state-of-the-art performance compared to other character-based models. Using a computational pragmatic model like ByteNet is essential when one does not have Google-level resources.

The ByteNet model and the semi-supervised approach have never been combined before. In particular, the original paper implementing the semi-supervised approach used the word-based Bahdanau Attention model, which is a drastically different than the ByteNet model.

% Importance of translation
% 1. European parlement % http://www.europarl.europa.eu/multilingualism/trade_of_translator_en.htm 
% 2. Not everybody has human translator, not feasiable to learn it all.
% 3. Indian as an example %https://blog.google/products/translate/making-internet-more-inclusive-india/
% 3b. Statistics for google translate % https://blog.google/products/translate/ten-years-of-google-translate/

% History (SMT) and recent efforts (NMT).
% 4. SMT or PBMT is the current approach.  % https://en.wikipedia.org/wiki/Statistical_machine_translation, % http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d3/data/pdf/anthology-PDF/J/J03/J03-1002.pdf
% 5. Issues: Whereas Phrase-Based Machine Translation (PBMT) breaks an input sentence into words and phrases to be translated largely independently, Neural Machine Translation (NMT) considers the entire input sentence as a unit for translation. %  https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
% 6. Google Neural Machine Translation % https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation


% Challenges for language pairs with little interaction.
% 7. most data wins
% 8. language pairs with little data is problematic
% 9. Idea, it is possible to learn a language without foreknowledge, Word2Vec % https://arxiv.org/pdf/1309.4168.pdf
% 10. Learn language separately and use bilingural to learn translation.

% Translation is a difficult problem, be pragmatic.
% 11. GAN is stil very difficult on text (https://arxiv.org/abs/1705.10929).
% 12. Be pragmatic, don't have google level resources.
% 13. ByteNet provides linear-time learning.
% 14. Character-based NMT advantages.
% 15. Semi-Supervised NMT is also very pragmatic.
% 16. ByteNet and Semi-Supervised NMT has never been combined before.

% Extra:
% http://www.cbc.ca/news/canada/british-columbia/tomatoes-google-kelowna-curtis-stone-1.3532564
