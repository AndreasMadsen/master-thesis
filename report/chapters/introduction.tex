%!TEX root = ../Thesis.tex
\chapter{Introduction}

In the European Parliament there are 23 officially spoken languages. In the European Parliament precise translation is essential for clear communication. Each the elected member is not expected to understand all 23 languages, and there isn't a common language that everyone speaks equally well, thus the European Parliament employs translators for translating between these languages \cite{europarl-translation}.

For the European Parliament it is possible to employ translation specialist, and in cases where there aren't someone to translate directly English, French and German are used as relay languages \cite{europarl-translation}. However translation specialists are not available to the public.

In Indian more than 400 million have internet access, and most of Indiaâ€™s online content is written in English. However, only 20\% of the Indian population speaks English. The nine most widely used languages, excluding English, in Indian are according to Google Hindi, Bengali, Marathi, Gujarati, Punjabi, Tamil, Telugu, Malayalam and Kannada. For translating between these languages and English Google have recently started to use Neural Machine Translation in their Google Translate service \cite{google-translate-india}.

Google Translate translates more than 100 billion words each day, for its 500 millions users \cite{google-translate-stats}. In the European Parliament and the United Nations automatic translation software is also used for assisting the specialists \cite{europarl-translation}. These tools have used, and are still using for the majority of languages, a technology called ``Statistical machine translation''.

Statistical machine translation (SMT) combines a probability model for the target language, as well as a probability model for mapping between the source and target language. Such a probability model can be a Hidden Markov Model and they will be fitted using a bilingual dataset \cite{smt-comparetive-study}. Previously these models have been word-based, allowing very limited context understand. Recently phrase-based machine translation (PBMT) have been introduced, this allows for much better translation of idioms or multi-word expressions than what has previously been possible. Phrase-based translation is currently the primary strategy used in machine translation. However, even phrase-based translation has limited understand of context and can't consider an entire sentence.

Recently advances has been made in applying neural networks to machine translation, this strategy is called Neural Machine Translation (NMT) and has been shown to outperform the current PBMT approaches \cite{google-translate-gnmt}. This approach is able to consider an entire sentence or more, and is thus able to understand the context of each word on a level that has not been previously possible.

Beyond being able to process entire sentences, neural machine translation is a more flexible approach than PBMT. As such the NMT strategy is highly relevant for language pairs that has previously been notoriously difficult, such as Chinese to English translation. In September 2016, Google announced that they now used neural machine translation for Chinese to English translation, they are calling this architecture GNMT.

Over the last year GNMT has been enabled for English, French, German, Spanish, Portuguese, Chinese, Japanese, Korean and Turkish, Russian, Hindi, Vietnamese, Hebrew and Arabic along with the Indian dialects. These languages are chosen either because there is a huge dataset or because they are notoriously difficult to translate using the existing PBMT strategy. In NMT the quality of the translation is often more dependent on how much data is used, than how advanced the neural architecture is.

In this thesis NMT will be used to provide German to English translation, and additional effort will be given to applying NMT to ``small'' bilingual dataset. The strategy for applying NMT to ``small'' datasets, is based on the fact that humans, especially babies, are capable of learning languages with out prior knowlege. It should thus be possible to use non-translated data (monolingural) to train the NMT model. An now almost classical example of using monolingural data to build a language model is the Word2Vec model, that uses the Wikipedia corpus for a single language to create word embeddings. These word embeddings have shown meaningful properties like synonyms being close to each other and relations like $king - man + woman \approx queen$ \cite{word2vec}.

The approach used in this thesis is based on the intuition that translating from for example German to English and then back to German again, should result in the same sentence. This approach does not require the correct English sentence to be known for all sentences, thus a monolingural German dataset can be used. The approach has been shown to yield some improvements \cite{semi-supervised-nmt}. This must of cause be used in combination with a bilingual dataset, otherwise the neural network will just learn the identity function. This combination of monolingural and bilingual data is called semi-supervised learning.

There are other approaches for semi-supervised learning, in particular Generative Adversarial Networks (GAN) has shown good results in computer vision. However for natural language processing such as NMT it still lags far behind likelihood based methods \cite{gan-on-nlp}.

The semi-supervised approach does on a theoretical level not depend on the neural translation model. A popular translation model is the Bahdanau Attention model that has shown state-of-the-art performance in machine translation \cite{bahdanau-2015-nmt}. However, the Bahdanau Attention model is a word-based model and for some languages, like German that likes to combine words, it can be an advantage to use character instead of words as input to the model. While word-based model are currently superior, using character instead of words eliminates the out-of-dictionary problem that often exist in word-based model.

The out-of-dictionary problem happens because a neural network only supports a fixed number of outputs, one must thus decide on a dictionary before training the model. Typically 80000 words are used as the dictionary size, if the dictionary becomes much larger it is not computational feasible to train the model. 80000 words is not enough to contain every streetname and rarely used words, an obvious solution is thus to use characters instead of words. A typical language will not use more than 300 different characters.

While it is possible to use the Bahdanau Attention model with characters as input, the sequences becomes much longer and this creates certain computational problems. Recently a different approach called ByteNet has been created, this model is specifically characters-based and solves the computational problems \cite{bytenet}. The ByteNet model promises high computational performance and achieves state-of-the-art performance compared to other characters-based models. Using a computational pragmatic model like ByteNet is essential when one does not have Google level resources.

The ByteNet model and the semi-supervised approach have never been combined before. In particular the original paper implementing the semi-supervised approach used the word-based Bahdanau Attention model, which is a drastically diffrent than the ByteNet model.

% Importance of translation
% 1. European parlement % http://www.europarl.europa.eu/multilingualism/trade_of_translator_en.htm 
% 2. Not everybody has human translator, not feasiable to learn it all.
% 3. Indian as an example %https://blog.google/products/translate/making-internet-more-inclusive-india/
% 3b. Statistics for google translate % https://blog.google/products/translate/ten-years-of-google-translate/

% History (SMT) and recent efforts (NMT).
% 4. SMT or PBMT is the current approach.  % https://en.wikipedia.org/wiki/Statistical_machine_translation, % http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d3/data/pdf/anthology-PDF/J/J03/J03-1002.pdf
% 5. Issues: Whereas Phrase-Based Machine Translation (PBMT) breaks an input sentence into words and phrases to be translated largely independently, Neural Machine Translation (NMT) considers the entire input sentence as a unit for translation. %  https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
% 6. Google Neural Machine Translation % https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation


% Challenges for language pairs with little interaction.
% 7. most data wins
% 8. language pairs with little data is problematic
% 9. Idea, it is possible to learn a language without foreknowledge, Word2Vec % https://arxiv.org/pdf/1309.4168.pdf
% 10. Learn language separately and use bilingural to learn translation.

% Translation is a difficult problem, be pragmatic.
% 11. GAN is stil very difficult on text (https://arxiv.org/abs/1705.10929).
% 12. Be pragmatic, don't have google level resources.
% 13. ByteNet provides linear-time learning.
% 14. Character-based NMT advantages.
% 15. Semi-Supervised NMT is also very pragmatic.
% 16. ByteNet and Semi-Supervised NMT has never been combined before.

% Extra:
% http://www.cbc.ca/news/canada/british-columbia/tomatoes-google-kelowna-curtis-stone-1.3532564
