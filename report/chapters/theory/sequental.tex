\section{Sequental Networks}

Traditional Feed Forward Neural Networks are only capable of taking a fixed size vector as input and outputting a fixed sized vector. While it is theoretically possible to express a sentense in fixed sized vector by using a lot of zero padding, it is not very pratical. A much better approach is to use sequental networks, these are able to take a variable length sequence of fixed sized vectors and outputting a sequence of fixed size vectors. There are diffrent variations on this idea that utilises diffrent strategies of aligning the input sequence with the output sequence. The most general appoach is called temporal classification \cite{alexgraves}, which allows any alignment between the input and target sequence. Temporal classification is necessary for language translation where the aligment often is unknown.

This class if problems are generally solved by creating a neural network that can fit the properbility function $P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots x_{|S|}\})$, where $y_t\ \forall t \in [1, |T|]$ is the target sequence and $|T|$ is the length of the target sequence. Similarly $x_t\ \forall t \in [1, |S|]$ is the input (source) sequence of length $|S|$. The cental idea is that the $P(y_t| \cdot)$ known the entire input sequence, but only the parts of the output sequence that came before $y_t$. This allows one to predict the entire sequence during inference, by iterating from $t = 1$ to $t = |T|$.

In training the loss is constructed by considering the joint properbility over all time steps \cite{alexgraves}:
\begin{equation}
P(y_1, \dots, y_{|T|}) = \prod_{t=1}^{|T|} P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots, x_{|S|}\})
\end{equation}

This construction is computationally convenient as it allows one to use log-properbilities instead of properbilities:
\begin{equation}
\log(P(y_1, \dots, y_{|T|})) = \sum_{t=1}^{|T|} \log(P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots, x_{|S|}\}))
\end{equation}

The ByteNet model, which will be discusses later in section \ref{sec:theory:bytenet}, is able to fit the properbility $P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots, x_{|S|}\})$. To understand the pros of ByteNet and how it differs from existing word-based neural translation models, a short introduction to two popular models, Sutskever 2014 \cite{sutskever-2014-nmt}, and Bahdanau 2015 \cite{bahdanau-2015-nmt} is given. Later in the ByteNet chapter the corns will be discussed.

\subsection{Sutskever Model}

\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
\text{encoding:} & \\
& \mathbf{h}_j = f(\mathbf{x}_j, \mathbf{h}_{j-1}) \quad \forall j \in [1, |S|] \\
\text{decoder:} & \\
&\mathbf{s}_i = \mathrm{g}(\mathbf{h}_{|S|}, \mathbf{s}_{i-1}) \\
&\mathbf{y}_i = \mathrm{softmax}(\mathbf{s}_i) \quad \forall i \in [1, |T|] 
\end{aligned}
\end{equation*}
\caption{The Sutskever 2014 model \cite{sutskever-2014-nmt}.}
\end{equationbox}

The Sutskever 2014 model \cite{sutskever-2014-nmt} was one of the first neural machine translation models to shows state-of-the-art performance on the WMT 2014 dataset.

The general idea is to encode a sequences of words using a recurrent neural network, the last encoder state is then used to initialize the decoder. The decoder iterates using the previously predicted word. While this approach is mathmatically elegant, encoding the source sentense ($S$) into a finite sized vector $h_{|S|}$ is in practice very difficult. The original implementation required a 8000 real valued dimentional vector for the sentence encoding. They also limited the source vocabulary to 16000 words and 8000 words for the target vocabulary.

Word-based neural machine translation models have shown to work well in practise, however they also have obviouse limitations. Words not in the vocabulary can't be translated, a common issue is names which for charecter-based models are very easy to translate because they require no translation. The softmax of a large vocabulary is also expensive, in the original Sutskever implementation they used 4 GPUs for just the softmax and 4 more GPUs for the rest of the network. This again can be solved by using charecter-based models because the ``vocabulary'' is just the diffrent letters, which there are a lot fewer off.

Using charecter instead of words in the Sutskever model may seam like a good idea at first, however the source and target sequences becomes much longer. Long sequences are difficult to encode and decode because the state is updated in each iteration, thus the state produced for the begining of the sentence is easily forgotten. In theory LSTM units solves this vanshing-moment issue, but in practise it still exists. In particullar Sutskever reversed the input sentence to get better performance, if there where no memory issues reversing the input sentence should have no effect.

\subsection{Bahdanau Attention Model}
\label{sec:theory:sequential:bahdanau}

\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
\text{encoding:} & \\
& \mathbf{h}_j = f(\mathbf{x}_j, \mathbf{h}_{j-1}) \quad \forall j \in [1, |S|] \\
\text{attention:} & \\
& e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j) \\
& \bm{\alpha}_i = \mathrm{sparsemax}(\mathbf{e}_i) \\
& \mathbf{c}_i = {\textstyle \sum_{t=1}^T} \alpha_{it} \mathbf{h}_t \\
\text{decoder:} & \\
&\mathbf{s}_i = \mathrm{g}(\mathbf{c}_i, \mathbf{s}_{i-1}) \\
&\mathbf{y}_i = \mathrm{softmax}(\mathbf{s}_i) \quad \forall i \in [1, |T|] 
\end{aligned}
\end{equation*}
\caption{The attention based Bahdanau 2015 model \cite{bahdanau-2015-nmt}.}
\end{equationbox}

Bahdanau et. al. solved this memory issue by letting the decoder look at selected parts of the encoding state sequence. It does this though what is called an attention mecanishim. The attention $\bm{\alpha}_i$ is a weight vector, that is used in a weighted mean calculation over the encoded states $\mathbf{h}_t$. The weights a calculated using a sub-network that depends on the previuse output state $\mathbf{s}_{i-1}$ and the encoding states. The weighted mean is then used to calculate the next output state $\mathbf{s}_{i}$.

The attention mecanishim essentially recalculate a new encoding vector for each output state. This creates what called a resolution preserving encoding, that is an encoding which size depends on the source sequence, this is diffrent from the Sutskever model that uses a fixed sized vector. The Bahdanau et. al model is words-based and achieved state-of-the-art like performance. On the surface there is nothing that prevents the Bahdanau et. al. model from being chatecter-based, but by looking at the computational complexity it becomes clear that using chatecters is not a viable solution.

In each iteration on the output sequence, the attention model needs to calculate the weighted mean over the encoding sequence, this takes $\mathcal{O}(|S|)$ time. These calculations can not be reused in the next iteration because they also depends on the output state from the previuse iteration $s_{i-1}$. The attention thus needs to be recaulated for each output state, resulting in a computational complexity of $\mathcal{O}(|S||T|)$.

Having this quadratic-like computational complexity $\mathcal{O}(|S||T|)$, means that using characters instead of words dramatically increases the running time.

