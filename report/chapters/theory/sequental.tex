\section{Sequential Networks}

Traditional Feed Forward Neural Networks are only capable of taking a fixed size vector as input and outputting a fixed sized vector. While it is theoretically possible to express a sentence in fixed sized vector by using a lot of zero padding, it is not very practical. A much better approach is to use sequential networks, these are able to take a variable length sequence of fixed sized vectors and outputting a sequence of fixed size vectors. There are different variations on this idea that utilizes different strategies of aligning the input sequence with the output sequence. The most general approach is called temporal classification \cite{alexgraves}, which allows any alignment between the input and target sequence. Temporal classification is necessary for language translation where the alignment often is unknown.

This class if problems are generally solved by creating a neural network that can fit the probability function $P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots x_{|S|}\})$, where $y_t\ \forall t \in [1, |T|]$ is the target sequence and $|T|$ is the length of the target sequence. Similarly $x_t\ \forall t \in [1, |S|]$ is the input (source) sequence of length $|S|$. The central idea is that the $P(y_t| \cdot)$ known the entire input sequence, but only the parts of the output sequence that came before $y_t$. This allows one to predict the entire sequence during inference, by iterating from $t = 1$ to $t = |T|$.

In training the loss is constructed by considering the joint probability over all time steps \cite{alexgraves}:
\begin{equation}
P(y_1, \dots, y_{|T|}) = \prod_{t=1}^{|T|} P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots, x_{|S|}\})
\end{equation}

This construction is computationally convenient as it allows one to use log-probabilities instead of probabilities:
\begin{equation}
\log(P(y_1, \dots, y_{|T|})) = \sum_{t=1}^{|T|} \log(P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots, x_{|S|}\}))
\end{equation}

The ByteNet model, which will be discusses later in section \ref{sec:theory:bytenet}, is able to fit the probability $P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots, x_{|S|}\})$. To understand the pros of ByteNet and how it differs from existing word-based neural translation models, a short introduction to two popular models, Sutskever 2014 \cite{sutskever-2014-nmt}, and Bahdanau 2015 \cite{bahdanau-2015-nmt} is given. Later in the ByteNet chapter the corns will be discussed.

\subsection{Sutskever Model}

\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
\text{encoding:} & \\
& \mathbf{h}_j = f(\mathbf{x}_j, \mathbf{h}_{j-1}) \quad \forall j \in [1, |S|] \\
\text{decoder:} & \\
&\mathbf{s}_i = \mathrm{g}(\mathbf{h}_{|S|}, \mathbf{s}_{i-1}) \\
&\mathbf{y}_i = \mathrm{softmax}(\mathbf{s}_i) \quad \forall i \in [1, |T|]
\end{aligned}
\end{equation*}
\caption{The Sutskever 2014 model \cite{sutskever-2014-nmt}.}
\end{equationbox}

The Sutskever 2014 model \cite{sutskever-2014-nmt} was one of the first neural machine translation models to shows state-of-the-art performance on the WMT 2014 dataset.

The general idea is to encode a sequences of words using a recurrent neural network, the last encoder state is then used to initialize the decoder. The decoder iterates using the previously predicted word. While this approach is mathematically elegant, encoding the source sentence ($S$) into a finite sized vector $h_{|S|}$ is in practice very difficult. The original implementation required a 8000 real valued dimensional vector for the sentence encoding. They also limited the source vocabulary to 16000 words and 8000 words for the target vocabulary.

Word-based neural machine translation models have shown to work well in practise, however they also have obvious limitations. Words not in the vocabulary can't be translated, a common issue is names which for character-based models are very easy to translate because they require no translation. The softmax of a large vocabulary is also expensive, in the original Sutskever implementation they used 4 GPUs for just the softmax and 4 more GPUs for the rest of the network. This again can be solved by using character-based models because the ``vocabulary'' is just the different letters, which there are a lot fewer off.

Using character instead of words in the Sutskever model may seam like a good idea at first, however the source and target sequences becomes much longer. Long sequences are difficult to encode and decode because the state is updated in each iteration, thus the state produced for the beginning of the sentence is easily forgotten. In theory LSTM units solves this vanishing-moment issue, but in practise it still exists. In particular Sutskever reversed the input sentence to get better performance, if there where no memory issues reversing the input sentence should have no effect.

\subsection{Bahdanau Attention Model}
\label{sec:theory:sequential:bahdanau}

\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
\text{encoding:} & \\
& \mathbf{h}_j = f(\mathbf{x}_j, \mathbf{h}_{j-1}) \quad \forall j \in [1, |S|] \\
\text{attention:} & \\
& e_{ij} = a(\mathbf{s}_{i-1}, \mathbf{h}_j) \\
& \bm{\alpha}_i = \mathrm{softmax}(\mathbf{e}_i) \\
& \mathbf{c}_i = {\textstyle \sum_{t=1}^T} \alpha_{it} \mathbf{h}_t \\
\text{decoder:} & \\
&\mathbf{s}_i = \mathrm{g}(\mathbf{c}_i, \mathbf{s}_{i-1}) \\
&\mathbf{y}_i = \mathrm{softmax}(\mathbf{s}_i) \quad \forall i \in [1, |T|]
\end{aligned}
\end{equation*}
\caption{The attention based Bahdanau 2015 model \cite{bahdanau-2015-nmt}.}
\end{equationbox}

Bahdanau et. al. solved this memory issue by letting the decoder look at selected parts of the encoding state sequence. It does this though what is called an attention mechanism. The attention $\bm{\alpha}_i$ is a weight vector, that is used in a weighted mean calculation over the encoded states $\mathbf{h}_t$. The weights a calculated using a sub-network that depends on the previous output state $\mathbf{s}_{i-1}$ and the encoding states. The weighted mean is then used to calculate the next output state $\mathbf{s}_{i}$.

The attention mechanism essentially recalculate a new encoding vector for each output state. This creates what called a resolution preserving encoding, that is an encoding which size depends on the source sequence, this is different from the Sutskever model that uses a fixed sized vector. The Bahdanau et. al model is words-based and achieved state-of-the-art like performance. On the surface there is nothing that prevents the Bahdanau et. al. model from being character-based, but by looking at the computational complexity it becomes clear that using characters is not a viable solution.

In each iteration on the output sequence, the attention model needs to calculate the weighted mean over the encoding sequence, this takes $\mathcal{O}(|S|)$ time. These calculations can not be reused in the next iteration because they also depends on the output state from the previous iteration $s_{i-1}$. The attention thus needs to be recalculated for each output state, resulting in a computational complexity of $\mathcal{O}(|S||T|)$.

Having this quadratic-like computational complexity $\mathcal{O}(|S||T|)$, means that using characters instead of words dramatically increases the running time.
