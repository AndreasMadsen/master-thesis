\section{Semi-Supervised Learning for NMT}
\subsection{Semi-Supervised loss}

\subsection{Beam Search}

Getting the $k$ most likely sequences when given a translation model $P(y_t | \{y_1, \dots, y_{t-1}\}, \{x_1, \dots, x_{|S|}\})$ is not a trivial task. By encoding $\{x_1, \dots, x_{|S|}\}$ one can calculate $P(y_1| \{x_1, \dots, x_{|S|}\})$ for each possible symbol that $y_1$ can attain. For each $y_1$ symbol, $P(y_2| \{y_1\}, \{x_1, \dots, x_{|S|}\})$ can be calculated for each possibol symbol that $y_2$ can attain. This recursion can be repeated until one has $k$ sequences that all ended with an \texttt{<EOS>} symbol and are more likely than all other sequences. This procedure works in theory but the running time and memory is exponential with the sequence length, thus this is not a very practical approach.

BeamSearch is a heuristic for getting the $k$ most likely sequences. On a theoretical level BeamSearch is contraitive to its fansy name fairly intuitive, the complexity lies in how it is implemented.

BeamSearch has a parameter called the \textit{beam size} (denoted $b$), this size determins the number of sequences that is keept track of. It limits the number of sequences (called \textit{paths}) by assigning each new path in the next iteration a score, the top $b$ new sequences are then keeped and used in the next iteration. To initialize the paths the top $b$ outputs from $P(y_1| \{x_1, \dots, x_{|S|}\})$ are used. After either a maximum number of iterations or when the top $k$ sequences in the the set of memorized paths (called the \textit{beam}) have ended with an \texttt{<EOS>} symbol, the BeamSearch algorithm can stop.

\todo[inline]{BeamSearch psudo code}

For $b = 1$ the BeamSearch algorithm becomes a completely greedy search that just takes the most likely symbol in each iteration. For $b = \infty$ BeamSearch becomes the complete search above that runs in exponential time. 