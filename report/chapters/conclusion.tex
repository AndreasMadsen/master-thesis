%!TEX root = ../Thesis.tex
\chapter{Conclusion}

Machine translation is a difficult and fast-moving field. Over the 6 months, this thesis has been carried out, several papers with new translation models have been published. Each model shows a new state-of-the-art performance over the previous models, on the WMT translation task \cite{bytenet, attention-is-all-you-need, tensor2tensor}. These models are created by Google, Microsoft, Facebook, etc., who have both computational resources and manpower to implement and experiment with a huge number of models. The goal and expectation of this thesis are thus not to compete with these models, but rather explore a new direction for neural machine translation, by using additional monolingual (unlabeled) data. However, to do this a fast and decent translation model is required, thus much effort has been put into the supervised model.

\paragraph{ByteNet} The ByteNet model was able to learn both the synthetic digits problem and memorize the WMT NewsTest dataset, this validates the implementation and capabilities of the ByteNet model. For the real problem where the Europarl v7 dataset is used, ByteNet managed to achieve a BLEU score of $7.44$ and produce reasonably good translations. This is far from the state-of-the-art model \cite{attention-is-all-you-need} or just a phase-based (PBMT) baseline model \cite{tensor2tensor}, but given the limited resources the results are rather good. The primary reason for why a better BLEU score wasn't achieved, is that training the ByteNet model is too time-consuming.

The original ByteNet paper including the latest revision did not disclose how much time was spent training or how many resources they used. From a scientific point of view, this is rather inadequate, especially because they created the model from a computational performance perspective. A recent blog post from Google published in June 2017, compares the different machine translation models and shows that ByteNet is actually one of the most computationally expensive models \cite{tensor2tensor}. Their comparison shows the ByteNet model requires ``8 days on 32 GPUs'' (equivalent to 256 computational days), while the recently published state-of-the-art model \cite{attention-is-all-you-need} uses ``3 days on 8 GPUs'' (equivalent to 24 computational days) \cite{tensor2tensor}. 

It is possible that if: the oscillation issues were solved, TensorFlow improves the execution speed with the XLA just-in-time (JIT) compiler \cite{google-xla}, and if CuDNN added support for dilated convolution and normalization, that ByteNet could run in reasonable time. However, it will likely take at least a year before TensorFlow and CuDNN are at this state.

\paragraph{Simplified ByteNet} Because ByteNet takes too much time to train, a simplified version of ByteNet was created. This model has 1/3 the layers and 5/9 the weights, while still maintaining the founding principals behind ByteNet and maintains the dimensional bottleneck. This was done by reducing the embedding dimensionality and removing the dimensional compression and decompression layers.

On the validation problems, this performed identically or slightly better compared to the normal ByteNet model. However, when trained on the Europarl v7 dataset the model showed severe overfitting and achieves a test BLEU score of only $0.55$. This indicates that the compression and decompression layers are somehow essential regularization components. In terms of time spend training, the simplified ByteNet model is only 20\% faster and converges correspondingly slower. Profiling the simplified ByteNet model, did unfortunately not reveal why the model is only 20\% faster, but did reveal that normalization is the primary bottleneck in terms of time spent.

The results indicate that simplifying ByteNet is not a direction worth exploring further, at least not for natural language translation. The approach might be valid for simpler problems, but for natural language translation, there are likely completely different model architectures that are better suited.

\paragraph{SELU ByteNet} Identifying that normalization is the primary computational bottleneck, suggests that if these layers can somehow be removed, the ByteNet model should perform significantly better. Training a non-normalized ByteNet model on even simple tasks converges very slowly, this is likely dude to a vanishing gradient issue. Recently, a paper showed that a special activation called SELU (Self-normalizing Exponential Linear Unit) can be used as a replacement for the typical ReLU and normalization combination. By using the SELU activation as a replacement for ReLU and normalization, the ByteNet model converges quickly on the validation tasks.

On the Europarl v7 dataset, the SELU ByteNet model does not converge. This appears to be caused by exploding gradients. Such issues could be solved by gradient clipping. However, the purpose of SELU is exactly to prevent exploding and vanishing gradients issues, thus gradient clipping is likely not a good strategy.

In terms of computational performance, the SELU ByteNet is much faster than both the normal ByteNet model and the simplified ByteNet model. On the Eurparl v7 problem the SELU ByteNet model runs 13 epochs in 18 hours, the normal ByteNet model uses 4 days on the same number of epochs. Profiling reveals that the time is primarily spent in the SELU activation function. This validates the known issue, where TensorFlow spends an unreasonably amount of time running simple element-wise operations \cite{google-xla}.

From a computational perspective, SELU ByteNet is promising, however as is, it is not suitable for learning translation on the Eurparl v7 dataset. If SELU ByteNet could be made to work, ByteNet would be a serious contender to other models in terms of training time.

\paragraph{Semi-Supervised ByteNet} Without a suitable fast translation model, it is not feasible to run the semi-supervised ByteNet model on the Europarl v7 dataset. This is because the semi-supervised model depends on two translation models, each running on translations produced by a BeamSearch algorithm. Thus, one should expect the semi-supervised model to take approximately 10 times longer to train. 

With the computational demands of ByteNet in mind, the semi-supervised ByteNet model was used on a synthetic problem. This problem maps spelled-out digits to digit symbols, for example, ``one zero four'' is translated to ``104''. On this problem, the semi-supervised ByteNet model showed improvement over both the supervised ByteNet model and an attention-based baseline model.

This validates the results from the original paper \cite{semi-supervised}, where the BLEU score could be improved by $1.5$ up to $3.5$. While such an improvement doesn't completely solve all issues, when training on language-pairs where the bilingual dataset is small, it could be an important component when the bilingual data isn't plentiful but the monolingual data is.

% While the semi-supervised model is not mathematically interesting, it is a pragmatic approach that works well with models that have fast/linear-time inference, such as ByteNet. However the ByteNet model is too slow for it to be relevant.

\paragraph{Future Works} With the recent results published by Google, showing that ByteNet is actually one of the slowest machine translation models \cite{tensor2tensor}, ByteNet is not a likely model for neural machine translation, and even less likely as a semi-supervised model. It is possible that the SELU ByteNet model could be tuned to work, in which case it might be a competing model. More time should be spent exploring possible methods of removing the normalization layers.

Recently a new paper showed that attention can be used instead of bi-directional RNN layers or hierarchical dilated convolution. Using this approach, they created a neural machine translation model that achieves a new state-of-the-art BLEU score on the en-de WMT NewsTest translation task. The model is also faster than many previous models, as it shares many of the founding principals of ByteNet. That is, resolution persistent encoding, parallelization over the sequences, and training in linear time are essential properties for a translation model \cite{attention-is-all-you-need}.

Such a translation model is close to ideal, to be used in combination with the semi-supervised approach. Although, because the model is based on attention, inference can't be done in linear time, unlike ByteNet. Linear time inferences may be essential for computing the unsupervised part of the loss-function. That being said, it is only the forward pass of one of the translation models that will run in quadratic time. Both backward passes and the other translation model will run in linear-time.

A different approach for making the semi-supervised model converge within reasonable time, is to initialize the weights with the learned weights from the two translation models it uses, where the translation models are pre-trained on a bilingual dataset. This would work as a pseudo-transfer learning, ``pseudo'' because the application domain is the same and it is the same models that are used. While this may improve convergence for a reasonable fast translation model, ByteNet is still too slow to be fully trained on just a single bilingual dataset.

Finally, one could look at a completely different approach for semi-supervised machine translation. In image processing, adversarial networks have been successfully used to ``translate'' between for example a zebra and a horse, without paired training data \cite{gan-image-translation}. A similar approach could perhaps be used in the field of natural language translation. However, the current state of adversarial networks for natural languages is still far behind compared to other applications \cite{gan-on-nlp}.