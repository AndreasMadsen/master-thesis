\chapter{Backward Pass}

\section{Softmax}
\label{appendix:backward-pass:softmax}

The forward pass for the softmax and the cross entropy loss is given by:

\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
y_k &= \frac{\exp(z_k)}{\sum_{k'=1}^K \exp(z_{k'})}, && \text{ where: } z_k=z_{h_{L+1}}, K = H_{L + 1} \\
\mathcal{L} &= - \sum_{k=1}^K t_k \ln(y_k)
\end{aligned}
\end{equation*}
\caption{Forward equations Cross Entropy loss with Softmax input.}
\end{equationbox}

The last delta ($\delta_{h_{L+1}}$) is what should be derived:
\begin{equation}
\delta_{h_{L + 1}} = \delta_k = \frac{\partial \mathcal{L}}{\partial z_k} = \sum_{k'=1}^K \frac{\partial \mathcal{L}}{\partial y_{k'}} \frac{\partial y_{k'}}{\partial z_k} = y_k - t_k
\label{appendix:backprop:softmax:bprop-delta-last}
\end{equation}

The first derivative $\frac{\partial \mathcal{L}}{\partial y_{k'}}$ is derived from the cross entropy equation:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial y_{k'}} = \frac{\partial}{\partial y_{k'}} \left(- \sum_{k''=1}^K t_{k''} \ln(y_{k''})\right) = -\frac{t_{k'}}{y_{k'}}
\label{appendix:backprop:softmax:bprop-Ldy}
\end{equation}

The other derivative $\frac{\partial y_{k'}}{\partial z_k}$ can be derived from the softmax function:
\begin{equation}
\begin{aligned}
\frac{\partial y_{k'}}{\partial z_k}
&= \frac{\partial}{\partial z_k} \frac{\exp(z_{k'})}{\sum_{k''=1}^K \exp(z_{k''})} \\
&= \frac{\frac{\partial}{\partial z_k} \exp(z_{k'})}{\sum_{k''=1}^K \exp(z_{k''})}
- \frac{\exp(z_{k'}) \frac{\partial}{\partial z_k} \sum_{k''=1}^K \exp(z_{k''})}{\left(\sum_{k''=1}^K \exp(z_{k''})\right)^2} \\
&= \frac{\frac{\partial}{\partial z_k} \exp(z_{k'})}{\sum_{k''=1}^K \exp(z_{k''})}
- \frac{\exp(z_{k'})}{\sum_{k''=1}^K \exp(z_{k''})} \frac{\frac{\partial}{\partial z_k} \sum_{k''=1}^K \exp(z_{k''})}{\sum_{k''=1}^K \exp(z_{k''})}
\end{aligned}
\end{equation}

Because of the difference in index, the first term is only not zero when $k = k'$, in which case $y_k$ is the derivative. It thus becomes useful to define:
\begin{equation}
\delta_{i,j} = \begin{cases}1& \text{when } i = j \\ 0 & \text{otherwise}\end{cases}
\end{equation}

Similarly in the second term $\frac{\partial}{\partial z_k} \exp(z_{k''})$ is zero except in the case where $k = k''$:
\begin{equation}
\frac{\partial y_{k'}}{\partial a_k} = \delta_{k, k'} y_k - y_{k'} y_k
\label{appendix:backprop:softmax:bprop-yda}
\end{equation}

The result from \eqref{appendix:backprop:softmax:bprop-Ldy} and \eqref{appendix:backprop:softmax:bprop-yda} is then combined into \eqref{appendix:backprop:softmax:bprop-delta-last}:
\begin{equation}
\begin{aligned}
\delta_{h_{L + 1}} = \delta_k &= \sum_{k'=1}^K -\frac{t_{k'}}{y_{k'}} \left( \delta_{k, k'} y_k - y_{k'} y_k \right) = \sum_{k'=1}^K -\frac{t_{k'}}{y_{k'}} \delta_{k, k'} y_k + \sum_{k'=1}^K \frac{t_{k'}}{y_{k'}} y_{k'} y_k \\
&= -\frac{t_k}{y_k} y_k + y_k \sum_{k'=1}^K t_{k'} = -t_k + y_k = y_k - t_k
\end{aligned}
\label{appendix:backprop:softmax:bprop-deltaKfinal}
\end{equation}

To get $\sum_{k'=1}^K t_{k'} = 1$ it's used that $\{ t_{k'} \}_{k'=1}^K$ is the target distribution and thus must sum to 1.

\section{Dilated Convolution}
\label{appendix:backward-pass:dilated-convolution}

\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
z_{h_\ell}(t) &= (a_{\ell-1} *_r w_{:, h_\ell})(t) = \sum_{h_{\ell-1}}^{H_{\ell-1}} \sum_{i} a_{h_{\ell-1}}(t + r\,i) w_{h_{\ell-1}, h_\ell}(i) \\
a_{h_\ell}(t) &= \theta(z_{h_\ell}(t))
\end{aligned}
\end{equation*}
\caption{Forward equations for Dilated Convolution.}
\end{equationbox}

For deriving the backward pass the padding will be ignored, this is not a big issue as the input image $a_{\ell-1}$ can easily be extended. It also turns out that the final derivative makes the generalization quite intuitive.

For the backward pass we just which to derive:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}(i)}
\end{equation}

The weight $w_{h_{\ell-1},h_\ell}(i)$ affects all time steps, thus the chain rule yields:
\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}(i)} &= \sum_{t} \frac{\partial \mathcal{L}}{\partial z_{h_\ell}(t)} \frac{z_{h_\ell}(t)}{\partial w_{h_{\ell-1},h_\ell}(i)} \\
&= \sum_{t} \delta_{h_\ell}(t) a_{h_{\ell-1}}(t + r\,i)
\end{aligned}
\end{equation}
\todo[inline]{somehow this can be rewritten to a convolution: $(\delta_{h_\ell} * rot(a_{h_{\ell-1}}))(i)$, see \url{https://grzegorzgwardys.wordpress.com/2016/04/22/8/}}

Here $\delta_{h_\ell}(t)$ is defined as:
\begin{equation}
\delta_{h_\ell}(t) \defeq \frac{\partial \mathcal{L}}{\partial z_{h_\ell}}(t)
\end{equation}

To calculate $\delta_{h_\ell}(t)$ the chain rule is used again, the first step is easy:
\begin{equation}
\delta_{h_\ell}(t) = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}(t)} = \frac{\partial \mathcal{L}}{\partial a_{h_\ell}(t)} \frac{\partial a_{h_\ell}(t)}{\partial z_{h_\ell}(t)} = \theta'(z_{h_\ell}(t)) \frac{\partial \mathcal{L}}{\partial a_{h_\ell}(t)}
\end{equation}

The next steps are more complicated, $a_{h_\ell}(t)$ affects $z_{h_{\ell+1}}(t')$ for all channels in the next layer, and all times $t'$ that are within the kernel width of $t$.
\begin{equation*}
\delta_{h_\ell}(t) = \theta'(z_{h_\ell}(t)) \frac{\partial \mathcal{L}}{\partial a_{h_\ell}(t)} = \theta'(z_{h_\ell}(t)) \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \sum_{i} \frac{\partial \mathcal{L}}{\partial z_{h_{\ell+1}}(t + r\, i)} \frac{\partial z_{h_{\ell+1}}(t + r\, i)}{\partial a_{h_\ell}(t)}
\end{equation*}

To derive $\frac{\partial z_{h_{\ell+1}}(t + r\, i)}{\partial a_{h_\ell}(t)}$ it helps to write out $z_{h_{\ell+1}}(t + r\, i)$.
\begin{equation}
z_{h_{\ell+1}}(t + r\, i) = (a_{\ell} * w_{:,h_\ell})(t + r\, i) = \sum_{h_\ell}^{H_\ell} \sum_{i'} a_{h_\ell}(t + r\, i + r\, i') w_{h_\ell, h_{\ell+1}}(i')
\end{equation}
From this one can observe that $t + r\, i + r\, i' = t$ only occurs when $i' = -i$, thus the $\delta_{h_\ell}(t)$ derivative becomes:
\begin{equation}
\begin{aligned}
\delta_{h_\ell}(t) &= \theta'(z_{h_\ell}(t)) \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \sum_{i} \frac{\partial \mathcal{L}}{\partial z_{h_{\ell+1}}(t + r\, i)} w_{h_\ell, h_{\ell+1}}(-i) \\
&= \theta'(z_{h_\ell}(t)) \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \sum_{i} \delta_{h_{\ell+1}}(t + r\, i) w_{h_\ell, h_{\ell+1}}(-i)
\end{aligned}
\end{equation}

This is actually a dilated convolution, just where the weight is flipped (rotated).
\begin{equation}
\delta_{h_\ell}(t) = \theta'(z_{h_\ell}(t)) (\delta_{\ell + 1} *_r \mathrm{rot}(w_{:, h_{\ell + 1}}))(t)
\end{equation}

This is the derivative for the dilated convolution. Because the derivative is a convolution itself, it naturally generalizes to any padding implementation. 

\clearpage
\section{Batch Normalization}
\label{appendix:backward-pass:batch-norm}

To derive the backward pass for batch normalization it helps to setup equations for $\mathbb{E}[z_{h_\ell}]$ and $\textsc{Var}[z_{h_\ell}]$. To do this we need to reintroduce the observation index this time it will be denoted with the superscript $(i)$. Similarly, the mini-batch will be denoted with $\mathcal{B}$. With these changes the the full forward pass can be written as:
\begin{equationbox}[H]
Activation:
\begin{equation*}
\begin{aligned}
z_{h_\ell}^{(i)} &= \sum_{h_{\ell-1}}^{H_{\ell-1}} w_{h_{\ell-1},h_\ell} a_{h_\ell-1}^{(i)} \\
\hat{z}_{h_\ell}^{(i)} &= \gamma_{h_\ell} \frac{z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}}{\sqrt{\sigma_{h_\ell}^{2,\mathcal{B}} + \epsilon}} + \beta_{h_\ell} \\
a_{h_\ell}^{(i)} &= \theta\left(\hat{z}_{h_\ell}^{(i)}\right)
\end{aligned}
\end{equation*}
Statistics:
\begin{equation*}
\begin{aligned}
\mu_{h_\ell}^{\mathcal{B}} &= \frac{1}{n} \sum_{i=1}^n z_{h_\ell}^{(i)} \\
\sigma_{h_\ell}^{2, \mathcal{B}} &= \frac{1}{n} \sum_{i=1}^n (z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}})^2
\end{aligned}
\end{equation*}
\caption{Forward equations for Batch Normalization.}
\end{equationbox}

Now that the forward pass is stated explicitly, the backward pass can be derived. For the backward pass we which to derive:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}}
\end{equation}

The gradient with respect to the weight $w_{h_{\ell-1},h_\ell}$ hasn't changed:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial z_{h_\ell}^{(i)}} \frac{\partial z_{h_\ell}^{(i)}}{\partial w_{h_{\ell-1},h_\ell}} =\sum_{i=1}^n \delta_{h_\ell}^{(i)} a_{h_{\ell-1}}^{(i)}
\end{equation}
Note that without batch normalization the sum over the observations does not directly appear in the gradients, but this is only because the observation index $i$ is omitted. In reality, it sums the gradients for each observation in the mini-batch.

The $\delta_{h_\ell}$ however has changed quite significantly:
\begin{equation}
\begin{aligned}
\delta_{h_\ell}^{(i)} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}^{(i)}} &= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\partial z_{h_\ell}^{(i)}} + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}} \frac{\partial \sigma_{h_\ell}^{2,\mathcal{B}}}{\partial z_{h_\ell}^{(i)}} + \frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}} \frac{\partial \mu_{h_\ell}^{\mathcal{B}}}{\partial z_{h_\ell}^{(i)}} \\
&= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\gamma_{h_\ell}}{\sqrt{\sigma_{h_\ell}^{2, \mathcal{B}} + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}} \frac{2}{n}\left(z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}\right) + \frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}} \frac{1}{n}
\end{aligned}
\label{eq:appendix:batch-norm:delta}
\end{equation}

From \eqref{eq:appendix:batch-norm:delta} the following derivatives also needs to be derived:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}}, \quad \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}}, \quad \frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}}
\end{equation}

As with everything else, this is done by using the chain rule:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} = \frac{\partial \mathcal{L}}{\partial a_{h_\ell}^{(i)}} \frac{\partial a_{h_\ell}^{(i)}}{\partial \hat{z}_{h_\ell}^{(i)}} = \theta'(\hat{z}_{h_\ell}^{(i)}) \frac{\partial\mathcal{L}}{\partial a_{h_\ell}^{(i)}} = \theta'(\hat{z}_{h_\ell}^{(i)}) \sum_{h_{\ell+1}}^{H_{\ell+1}} \delta_{h_{\ell+1}}^{(i)} w_{h_\ell, h_{\ell+1}}
\end{equation}
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h\ell}^{(i)}}{\partial \sigma_{h_\ell}^{2, \mathcal{B}}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \gamma_{h_\ell} \left(z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}\right) \left(-\frac{1}{2}\right) \left(\sigma_{h_\ell}^{\mathcal{B}} + \epsilon\right)^{-\frac{2}{3}}
\end{equation}
\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}} &= \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\mu_{h_\ell}^{\mathcal{B}}} + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2, \mathcal{B}}} \frac{\partial \sigma_{h_\ell}^{2, \mathcal{B}}}{\mu_{h_\ell}^{\mathcal{B}}} \\
&= \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \left(- \frac{1}{\sqrt{\sigma_{h_\ell}^{2, \mathcal{B}} + \epsilon}}\right) + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2, \mathcal{B}}} \frac{1}{n} \sum_{i=1}^n -2 \left(z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}\right)
\end{aligned}
\end{equation}

Finally, the gradient with respect to $\gamma_{h_\ell}$ and $\beta_{h_\ell}$ can be derived:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\partial \gamma_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \left(\frac{z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}}{\sqrt{\sigma_{h_\ell}^{2, \mathcal{B}} + \epsilon}} \right)
\end{equation}
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\partial \beta_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}}
\end{equation}

These only depend on $\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}}$ which has already been derived.

\clearpage
\section{Layer Normalization}

Let's repeat the forward pass:
\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
z_{h_\ell} &= \sum_{h_{\ell-1}}^{H_{\ell-1}} w_{h_{\ell-1},h_\ell} a_{h_\ell-1} \\
\hat{z}_{h_\ell} &= \gamma_{h_\ell} \frac{z_{h_\ell} - \mu_{\ell}}{\sqrt{\sigma_{\ell}^2 + \epsilon}} + \beta_{h_\ell} \\
a_{h_\ell} &= \theta\left(\hat{z}_{h_\ell}\right)
\end{aligned}
\end{equation*}
Statistics:
\begin{equation*}
\begin{aligned}
\mu_{\ell} &= \frac{1}{H_\ell} \sum_{h_\ell}^{H_\ell} z_{h_\ell} \\
\sigma_{\ell}^2 &= \frac{1}{H_\ell} \sum_{h_\ell}^{H_\ell} (z_{h_\ell} - \mu_{\ell})^2
\end{aligned}
\end{equation*}
\caption{Forward equations for Layer Normalization.}
\end{equationbox}

For the backward pass we which to derive:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}}
\end{equation}

The gradient with respect to the weight $w_{h_{\ell-1},h_\ell}$ is the same:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial w_{h_{\ell-1},h_\ell}} = \delta_{h_\ell} a_{h_{\ell-1}}
\end{equation}

Like in Batch Normalization $\delta_{h_\ell}$ has changed quite significantly:

\begin{equation}
\begin{aligned}
\delta_{h_\ell} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} &= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial z_{h_\ell}} + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{\partial \sigma_{\ell}^2}{\partial z_{h_\ell}} + \frac{\partial \mathcal{L}}{\partial \mu_{\ell}} \frac{\partial \mu_{\ell}}{\partial z_{h_\ell}} \\
&= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\gamma_{h_\ell}}{\sqrt{\sigma_{\ell}^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{2}{H_\ell}\left(z_{h_\ell} - \mu_{\ell}\right) + \frac{\partial \mathcal{L}}{\partial \mu_{\ell}} \frac{1}{H_\ell}
\end{aligned}
\label{eq:appendix:layer-norm:delta}
\end{equation}

From \eqref{eq:appendix:layer-norm:delta} the following derivatives also needs to be derived:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}}, \quad \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2}, \quad \frac{\partial \mathcal{L}}{\partial \mu_{\ell}}
\end{equation}

As with everything else this is done by using the chain rule:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial a_{h_\ell}} \frac{\partial a_{h_\ell}}{\partial \hat{z}_{h_\ell}} = \theta'(\hat{z}_{h_\ell}) \frac{\partial \mathcal{L}}{\partial a_{h_\ell}} = \theta'(\hat{z}_{h_\ell}) \sum_{h_{\ell+1}}^{H_{\ell+1}} \delta_{h_{\ell+1}} w_{h_\ell, h_{\ell+1}}
\end{equation}

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \sigma^2_\ell} = \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \sigma_\ell^2} = \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \gamma_{h_\ell} (z_{h_\ell} - \mu_\ell) \left(- \frac{1}{2}\right) \left(\sigma_\ell^2 + \epsilon\right)^{-\frac{2}{3}}
\end{equation}

\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \mu_\ell} &= \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \mu_\ell} + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{\partial \sigma_\ell^2}{\partial \mu_\ell} \\
&= \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \left(- \frac{1}{\sqrt{\sigma_\ell^2 + \epsilon}}\right) + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{1}{H_\ell} \sum_{h_\ell}^{H_\ell} -2 \left(z_{h_\ell} - \mu_\ell\right)
\end{aligned}
\end{equation}

Finally, the gradient with respect to $\gamma_{h_\ell}$ and $\beta_{h_\ell}$ can be derived:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \gamma_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \left(\frac{z_{h_\ell} - \mu_\ell}{\sqrt{\sigma_\ell^2 + \epsilon}}\right)
\end{equation}

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \beta_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}}
\end{equation}

These only depend on $\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}}$ which has already been derived.

\section{Semi-Supervised Marginalization}
\label{appendix:backward-pass:semi-sum}

\begin{equationbox}[H]
\begin{equation*}
\mathcal{L} = -\log(P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta}))
\end{equation*}
Monolingural translation model:
\begin{equation*}
P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta}) = \sum_{\mathbf{x}} P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta}) P(\mathbf{x}|\mathbf{y}, \overleftarrow{\boldsymbol\theta})
\end{equation*}
\caption{Loss function for the semi-supervised model.}
\end{equationbox}

To optimize the loss the gradient with respect to $\overrightarrow{\boldsymbol\theta}$ is calculated. The gradient with respect to $\overleftarrow{\boldsymbol\theta}$ is also needed, but that is symmetrically similar to the $\overrightarrow{\boldsymbol\theta}$ gradient.
\begin{equation}
\frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} \mathcal{L} = \frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} -\log(P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta}))
\end{equation}

Start out by differentiating the $\log(\cdot)$ function and insert the expression for $P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta})$:
\begin{equation}
\begin{aligned}
\frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} -\log(P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta}))
&= - \frac{\frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta})}{P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta})} \\
&= - \frac{\sum_{\mathbf{x}} \frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta}) P(\mathbf{x}|\mathbf{y}, \overleftarrow{\boldsymbol\theta})}{\sum_{\mathbf{x}} P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta}) P(\mathbf{x}|\mathbf{y}, \overleftarrow{\boldsymbol\theta})}
\end{aligned}
\end{equation}

The identity $\frac{\partial f({\boldsymbol\theta})}{\partial {\boldsymbol\theta}} = f({\boldsymbol\theta}) \frac{\partial \log(f({\boldsymbol\theta}))}{\partial {\boldsymbol\theta}}$ is then applied.
\begin{equation}
\frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} -\log(P(\mathbf{y}'|\mathbf{y}, \overrightarrow{\boldsymbol\theta}, \overleftarrow{\boldsymbol\theta})) = - \frac{\sum_{\mathbf{x}} P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta}) P(\mathbf{x}|\mathbf{y}, \overleftarrow{\boldsymbol\theta}) \frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} \log(P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta})) }{\sum_{\mathbf{x}} P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta}) P(\mathbf{x}|\mathbf{y}, \overleftarrow{\boldsymbol\theta})}
\end{equation}

Calculating $\frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} \log(P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta}))$ is more numerically stable than calculating $\frac{\partial}{\partial \overrightarrow{\boldsymbol\theta}} P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta})$ directly. Furthermore, since $P(\mathbf{y}'|\mathbf{x}, \overrightarrow{\boldsymbol\theta})$ already has to be calculated there is also no loss in performance.
