\chapter{Backward Pass}

\section{Dilated Convolution}
\label{appendix:backward-pass:dilated-convolution}

\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
z_{h_\ell}(t) &= (a_{\ell-1} *_r w_{:, h_\ell})(t) = \sum_{h_{\ell-1}}^{H_{\ell-1}} \sum_{i} a_{h_{\ell-1}}(t + r\,i) w_{h_{\ell-1}, h_\ell}(i) \\
a_{h_\ell}(t) &= \theta(z_{h_\ell}(t))
\end{aligned}
\end{equation*}
\caption{Forward equations for Dilated Convolution.}
\end{equationbox}

For deriving the backward pass the padding will be ignored, this is not a big issue as the input image $a_{\ell-1}$ can easily be extended. It also turns out that the final derivative makes the generalization quite intuitive.

For the backward pass we just which to derive:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}(i)}
\end{equation}

The weight $w_{h_{\ell-1},h_\ell}(i)$ affects all time steps, thus the chain rule yields:
\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}(i)} &= \sum_{t} \frac{\partial \mathcal{L}}{\partial z_{h_\ell}(t)} \frac{z_{h_\ell}(t)}{\partial w_{h_{\ell-1},h_\ell}(i)} \\
&= \sum_{t} \delta_{h_\ell}(t) a_{h_{\ell-1}}(t + r\,i)
\end{aligned}
\end{equation}
\todo[inline]{somehow this can be rewritten to a convolution: $(\delta_{h_\ell} * rot(a_{h_{\ell-1}}))(i)$, see \url{https://grzegorzgwardys.wordpress.com/2016/04/22/8/}}

Here $\delta_{h_\ell}(t)$ is defined as:
\begin{equation}
\delta_{h_\ell}(t) \defeq \frac{\partial \mathcal{L}}{\partial z_{h_\ell}}(t)
\end{equation}

To calculate $\delta_{h_\ell}(t)$ the chain rule is used again, the first step is easy:
\begin{equation}
\delta_{h_\ell}(t) = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}(t)} = \frac{\partial \mathcal{L}}{\partial a_{h_\ell}(t)} \frac{\partial a_{h_\ell}(t)}{\partial z_{h_\ell}(t)} = \theta'(z_{h_\ell}(t)) \frac{\partial \mathcal{L}}{\partial a_{h_\ell}(t)}
\end{equation}

The next steps are more complicated, $a_{h_\ell}(t)$ affects $z_{h_{\ell+1}}(t')$ for all channels in the next layer, and all times $t'$ that are within the kernel width of $t$.
\begin{equation*}
\delta_{h_\ell}(t) = \theta'(z_{h_\ell}(t)) \frac{\partial \mathcal{L}}{\partial a_{h_\ell}(t)} = \theta'(z_{h_\ell}(t)) \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \sum_{i} \frac{\partial \mathcal{L}}{\partial z_{h_{\ell+1}}(t + r\, i)} \frac{\partial z_{h_{\ell+1}}(t + r\, i)}{\partial a_{h_\ell}(t)}
\end{equation*}

To derive $\frac{\partial z_{h_{\ell+1}}(t + r\, i)}{\partial a_{h_\ell}(t)}$ it helps to write out $z_{h_{\ell+1}}(t + r\, i)$.
\begin{equation}
z_{h_{\ell+1}}(t + r\, i) = (a_{\ell} * w_{:,h_\ell})(t + r\, i) = \sum_{h_\ell}^{H_\ell} \sum_{i'} a_{h_\ell}(t + r\, i + r\, i') w_{h_\ell, h_{\ell+1}}(i')
\end{equation}
From this one can observe that $t + r\, i + r\, i' = t$ only occurs when $i' = -i$, thus the $\delta_{h_\ell}(t)$ derivative becomes:
\begin{equation}
\begin{aligned}
\delta_{h_\ell}(t) &= \theta'(z_{h_\ell}(t)) \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \sum_{i} \frac{\partial \mathcal{L}}{\partial z_{h_{\ell+1}}(t + r\, i)} w_{h_\ell, h_{\ell+1}}(-i) \\
&= \theta'(z_{h_\ell}(t)) \sum_{h_{\ell+1}=1}^{H_{\ell+1}} \sum_{i} \delta_{h_{\ell+1}}(t + r\, i) w_{h_\ell, h_{\ell+1}}(-i)
\end{aligned}
\end{equation}

This is actually a dilated convolution, just where the weight is flipped (rotated).
\begin{equation}
\delta_{h_\ell}(t) = \theta'(z_{h_\ell}(t)) (\delta_{\ell + 1} *_r \mathrm{rot}(w_{:, h_{\ell + 1}}))(t)
\end{equation}

This is the derivative for the dilated convolution. Because the derivative is a convolution itself, it naturally generalizes to any padding implementation. 

\clearpage
\section{Batch Normalization}
\label{appendix:backward-pass:batch-norm}

To derive the backward pass for batch normalization it helps to setup equations for $\mathbb{E}[z_{h_\ell}]$ and $\textsc{Var}[z_{h_\ell}]$. To do this we need to reintroduce the observation index this time it will be denoted with the superscript $(i)$. Similarly, the mini-batch will be denoted with $\mathcal{B}$. With these changes the the full forward pass can be written as:
\begin{equationbox}[H]
Activation:
\begin{equation*}
\begin{aligned}
z_{h_\ell}^{(i)} &= \sum_{h_{\ell-1}}^{H_{\ell-1}} w_{h_{\ell-1},h_\ell} a_{h_\ell-1}^{(i)} \\
\hat{z}_{h_\ell}^{(i)} &= \gamma_{h_\ell} \frac{z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}}{\sqrt{\sigma_{h_\ell}^{2,\mathcal{B}} + \epsilon}} + \beta_{h_\ell} \\
a_{h_\ell}^{(i)} &= \theta\left(\hat{z}_{h_\ell}^{(i)}\right)
\end{aligned}
\end{equation*}
Statistics:
\begin{equation*}
\begin{aligned}
\mu_{h_\ell}^{\mathcal{B}} &= \frac{1}{n} \sum_{i=1}^n z_{h_\ell}^{(i)} \\
\sigma_{h_\ell}^{2, \mathcal{B}} &= \frac{1}{n} \sum_{i=1}^n (z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}})^2
\end{aligned}
\end{equation*}
\caption{Forward equations for Batch Normalization.}
\end{equationbox}

Now that the forward pass is stated explicitly, the backward pass can be derived. For the backward pass we which to derive:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}}
\end{equation}

The gradient with respect to the weight $w_{h_{\ell-1},h_\ell}$ hasn't changed:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial z_{h_\ell}^{(i)}} \frac{\partial z_{h_\ell}^{(i)}}{\partial w_{h_{\ell-1},h_\ell}} =\sum_{i=1}^n \delta_{h_\ell}^{(i)} a_{h_{\ell-1}}^{(i)}
\end{equation}
Note that without batch normalization the sum over the observations does not directly appear in the gradients, but this is only because the observation index $i$ is omitted. In reality, it sums the gradients for each observation in the mini-batch.

The $\delta_{h_\ell}$ however has changed quite significantly:
\begin{equation}
\begin{aligned}
\delta_{h_\ell}^{(i)} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}^{(i)}} &= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\partial z_{h_\ell}^{(i)}} + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}} \frac{\partial \sigma_{h_\ell}^{2,\mathcal{B}}}{\partial z_{h_\ell}^{(i)}} + \frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}} \frac{\partial \mu_{h_\ell}^{\mathcal{B}}}{\partial z_{h_\ell}^{(i)}} \\
&= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\gamma_{h_\ell}}{\sqrt{\sigma_{h_\ell}^{2, \mathcal{B}} + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}} \frac{2}{n}\left(z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}\right) + \frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}} \frac{1}{n}
\end{aligned}
\label{eq:appendix:batch-norm:delta}
\end{equation}

From \eqref{eq:appendix:batch-norm:delta} the following derivatives also needs to be derived:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}}, \quad \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}}, \quad \frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}}
\end{equation}

As with everything else, this is done by using the chain rule:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} = \frac{\partial \mathcal{L}}{\partial a_{h_\ell}^{(i)}} \frac{\partial a_{h_\ell}^{(i)}}{\partial \hat{z}_{h_\ell}^{(i)}} = \theta'(\hat{z}_{h_\ell}^{(i)}) \frac{\partial\mathcal{L}}{\partial a_{h_\ell}^{(i)}} = \theta'(\hat{z}_{h_\ell}^{(i)}) \sum_{h_{\ell+1}}^{H_{\ell+1}} \delta_{h_{\ell+1}}^{(i)} w_{h_\ell, h_{\ell+1}}
\end{equation}
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2,\mathcal{B}}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h\ell}^{(i)}}{\partial \sigma_{h_\ell}^{2, \mathcal{B}}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \gamma_{h_\ell} \left(z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}\right) \left(-\frac{1}{2}\right) \left(\sigma_{h_\ell}^{\mathcal{B}} + \epsilon\right)^{-\frac{2}{3}}
\end{equation}
\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \mu_{h_\ell}^{\mathcal{B}}} &= \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\mu_{h_\ell}^{\mathcal{B}}} + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2, \mathcal{B}}} \frac{\partial \sigma_{h_\ell}^{2, \mathcal{B}}}{\mu_{h_\ell}^{\mathcal{B}}} \\
&= \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \left(- \frac{1}{\sqrt{\sigma_{h_\ell}^{2, \mathcal{B}} + \epsilon}}\right) + \frac{\partial \mathcal{L}}{\partial \sigma_{h_\ell}^{2, \mathcal{B}}} \frac{1}{n} \sum_{i=1}^n -2 \left(z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}\right)
\end{aligned}
\end{equation}

Finally, the gradient with respect to $\gamma_{h_\ell}$ and $\beta_{h_\ell}$ can be derived:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\partial \gamma_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \left(\frac{z_{h_\ell}^{(i)} - \mu_{h_\ell}^{\mathcal{B}}}{\sqrt{\sigma_{h_\ell}^{2, \mathcal{B}} + \epsilon}} \right)
\end{equation}
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}} \frac{\partial \hat{z}_{h_\ell}^{(i)}}{\partial \beta_{h_\ell}} = \sum_{i=1}^n \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}}
\end{equation}

These only depend on $\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}^{(i)}}$ which has already been derived.

\clearpage
\section{Layer Normalization}

Let's repeat the forward pass:
\begin{equationbox}[H]
\begin{equation*}
\begin{aligned}
z_{h_\ell} &= \sum_{h_{\ell-1}}^{H_{\ell-1}} w_{h_{\ell-1},h_\ell} a_{h_\ell-1} \\
\hat{z}_{h_\ell} &= \gamma_{h_\ell} \frac{z_{h_\ell} - \mu_{\ell}}{\sqrt{\sigma_{\ell}^2 + \epsilon}} + \beta_{h_\ell} \\
a_{h_\ell} &= \theta\left(\hat{z}_{h_\ell}\right)
\end{aligned}
\end{equation*}
Statistics:
\begin{equation*}
\begin{aligned}
\mu_{\ell} &= \frac{1}{H_\ell} \sum_{h_\ell}^{H_\ell} z_{h_\ell} \\
\sigma_{\ell}^2 &= \frac{1}{H_\ell} \sum_{h_\ell}^{H_\ell} (z_{h_\ell} - \mu_{\ell})^2
\end{aligned}
\end{equation*}
\caption{Forward equations for Layer Normalization.}
\end{equationbox}

For the backward pass we which to derive:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}},\quad \frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}}
\end{equation}

The gradient with respect to the weight $w_{h_{\ell-1},h_\ell}$ is the same:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_{h_{\ell-1},h_\ell}} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} \frac{\partial z_{h_\ell}}{\partial w_{h_{\ell-1},h_\ell}} = \delta_{h_\ell} a_{h_{\ell-1}}
\end{equation}

Like in Batch Normalization $\delta_{h_\ell}$ has changed quite significantly:

\begin{equation}
\begin{aligned}
\delta_{h_\ell} = \frac{\partial \mathcal{L}}{\partial z_{h_\ell}} &= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial z_{h_\ell}} + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{\partial \sigma_{\ell}^2}{\partial z_{h_\ell}} + \frac{\partial \mathcal{L}}{\partial \mu_{\ell}} \frac{\partial \mu_{\ell}}{\partial z_{h_\ell}} \\
&= \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\gamma_{h_\ell}}{\sqrt{\sigma_{\ell}^2 + \epsilon}} + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{2}{H_\ell}\left(z_{h_\ell} - \mu_{\ell}\right) + \frac{\partial \mathcal{L}}{\partial \mu_{\ell}} \frac{1}{H_\ell}
\end{aligned}
\label{eq:appendix:layer-norm:delta}
\end{equation}

From \eqref{eq:appendix:layer-norm:delta} the following derivatives also needs to be derived:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}}, \quad \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2}, \quad \frac{\partial \mathcal{L}}{\partial \mu_{\ell}}
\end{equation}

As with everything else this is done by using the chain rule:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial a_{h_\ell}} \frac{\partial a_{h_\ell}}{\partial \hat{z}_{h_\ell}} = \theta'(\hat{z}_{h_\ell}) \frac{\partial \mathcal{L}}{\partial a_{h_\ell}} = \theta'(\hat{z}_{h_\ell}) \sum_{h_{\ell+1}}^{H_{\ell+1}} \delta_{h_{\ell+1}} w_{h_\ell, h_{\ell+1}}
\end{equation}

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \sigma^2_\ell} = \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \sigma_\ell^2} = \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \gamma_{h_\ell} (z_{h_\ell} - \mu_\ell) \left(- \frac{1}{2}\right) \left(\sigma_\ell^2 + \epsilon\right)^{-\frac{2}{3}}
\end{equation}

\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \mu_\ell} &= \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \mu_\ell} + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{\partial \sigma_\ell^2}{\partial \mu_\ell} \\
&= \sum_{h_\ell}^{H_\ell} \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \left(- \frac{1}{\sqrt{\sigma_\ell^2 + \epsilon}}\right) + \frac{\partial \mathcal{L}}{\partial \sigma_{\ell}^2} \frac{1}{H_\ell} \sum_{h_\ell}^{H_\ell} -2 \left(z_{h_\ell} - \mu_\ell\right)
\end{aligned}
\end{equation}

Finally, the gradient with respect to $\gamma_{h_\ell}$ and $\beta_{h_\ell}$ can be derived:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \gamma_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \gamma_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \left(\frac{z_{h_\ell} - \mu_\ell}{\sqrt{\sigma_\ell^2 + \epsilon}}\right)
\end{equation}

\begin{equation}
\frac{\partial \mathcal{L}}{\partial \beta_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}} \frac{\partial \hat{z}_{h_\ell}}{\partial \beta_{h_\ell}} = \frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}}
\end{equation}

These only depend on $\frac{\partial \mathcal{L}}{\partial \hat{z}_{h_\ell}}$ which has already been derived.